{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier v2\n",
    "This notebook serves as my attempt at implementing deep learning to determine not only whether an image is cloudy, but also to determine (with some measure of accuracy) the number of cloudy pixels in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "The theory behind this neural network is outlined in a 2017 paper that serves as a written form of a presentation at the 8th International Conference of Pattern Recognition Systems, given in Madrid, Spain. The paper can be found in French archives (as the authors are all French, at French institutions) here: https://hal.archives-ouvertes.fr/hal-01783857/document. The paper is entitled *Deep learning for cloud detection*.\n",
    "\n",
    "As a general overview the theory is to apply the CNN to individual pixels, rather than an image as a whole. In Image Classifier v1 I aimed simply to use resnet34 to classify if an image was cloudy or not. Here I define a custom network (CloudNet) originally designed to be in the same structure as the network outlined in the paper on page 7.\n",
    "\n",
    "32x32 patches of an image surrounding a central pixel are passed to a neural network which attempts to learn how to classify the central pixel from the features that appear in the patch. This requires more work than my previous efforts, as it requires the entire image to have a \"cloud mask\" which indicates whether a pixel is part of a cloud or not. I expand upon this in more detail below.\n",
    "\n",
    "CloudNet is defined as a multi layer neural network that takes a 32x32x3 input patch, where the three channels are RGB. This patch is convoled with a 5x5 convolution filter to a 32x32 image with 64 channels, which then is pooled to a 16x16x64 patch. This is again convolved with a 5x5 filter, but remains at 64 channels. It is then pooled again, convolved once more, then pooled. The result is a 4x4 patch with 64 channels. This is linearized and reduced from 1024 parameters down to 384, then 192, and then the output 2. In the classification scheme, 1 reprsents a pixel that is a cloud, and 0 represents a non-cloud pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Methods\n",
    "An alternative convolutional neural network for finding clouds was proposed in a 2018 paper, *Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors*, which can be found here: https://arxiv.org/pdf/1810.05801.pdf. This network is vastly more complicated, although according to the paper, much more accurate. I provide it here as a possible future outlet of exploration.\n",
    "\n",
    "## Pitfalls\n",
    "Both of these algorithms were originally designed for satellite imagery of the Earth. Time will tell how accurate they are for pictures of the sky, arguably the exact opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CloudNet, self).__init__()\n",
    "        # 3 input channels (RGB), 64 output channels, 5x5 kernel\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        # Pools the 32x32 patch down to 16x16\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        # Pools to 8x8\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        # Pools to 4x4\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Linear operations\n",
    "        # First input should be 64 channels in 4x4 \"images\" due to the triple pooling.\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 384)\n",
    "        self.fc2 = nn.Linear(384, 192)\n",
    "        self.fc3 = nn.Linear(192, 2)\n",
    "        # 2 outputs, 1=cloud, 0=not\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # view essentially resizes a tensor, -1 means it'll infer that dimension\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe a major benefit in using a CNN this way presents itself readily in it's flexibility. In simply classifying an image as \"cloudy\" or \"non-cloudy\" the network is restricted to the 0.3s images alone, and would have to be greatly modified to fit the 6s images. By analyzing on a pixel level, I believe that the same network can be retrained on the 6s image dataset, and that the only change to the resulting network will be the weights of the underlying parameters. \n",
    "\n",
    "A further plus is given in that this allows us to determine the exact cloudiness of an image by simply running it through the network. On a GPU this would be a trivial amount of time. This should vastly improve on the previous method of determinig \"cloudiness,\" which used only the brightness of a pixel and could thus only be applied to 0.3s images. It may also misidentify pixels in much greater numbers, due to fluctuations in night sky brightness and camera effects. That, in turn, could strenghthen the correlation with the fraction of the night the dome is closed. The network could be expanded to predict this directly, although that is optimistic. This, of course, all assuming that this network actually works, and I haven't written this fun explanation for nothing :). Quite honestly, I'm writing all of this after defining the network and confirming that it works on a (1,3,32,32) tensor, but without actually running an image through it. \n",
    "\n",
    "In theory, since you could use the network on the entire night you'd know how many pixels over the entire night are clouds, and how many are not, and you'd know the fraction of that night that the dome was closed. You could add this as another layer in the network. In theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a list of pixels to run through the network. It uses a full mask, which sets all pixels ourside the circular image to 1, and all the pixels inside that are not horizon objects to 0. These pixels set to 0 are the ones we want to analyze. We need to ensure also that the pixels are at least 16 pixels from the edges of the image since we analyze 32x32 patches surrounding each pixel we are analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Images/Mask.png\n",
      "Saved: Images/Mask.png\n"
     ]
    }
   ],
   "source": [
    "# The center of the image.\n",
    "center = (256, 252)\n",
    "\n",
    "pixels = []\n",
    "\n",
    "msk = mask.generate_full_mask(True)\n",
    "# We need to save the full mask since generate_full_mask doesn't save the mask.\n",
    "mask.save_mask(msk)\n",
    "loc = os.path.join('Images', 'Mask.png')\n",
    "img = np.asarray(Image.open(loc).convert('L'))\n",
    "\n",
    "for row in range(0, img.shape[0]):\n",
    "    for column in range(0, img.shape[1]):\n",
    "        if img[row, column] == 0 and row > 16 and column > 16 and row < 496 and column < 496:\n",
    "            pixels.append([row, column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here define a Dataset object that loads an image and then creates 32x32 patches surrounding each pixel that will be analyzed in the network. It additionally loads a binary image where the cloud pixels are set to 1 and the non cloud images are set to 0. This image is used to determine the label for each 32x32 patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelDataset(Dataset):\n",
    "    def __init__(self, name, pixels, transform=None):\n",
    "        self.name = name\n",
    "        self.pixels = pixels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pixels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # This one loads the image itself\n",
    "        # https://github.com/python-pillow/Pillow/issues/835\n",
    "        # Thanks to https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
    "        # For this solution.\n",
    "        loc = os.path.join('Images', *['data', 'train', 'new', self.name])\n",
    "        with open(loc, 'rb') as f:\n",
    "            im = np.asarray(Image.open(f).convert('RGB'))\n",
    "        \n",
    "        # This one loads the binary image that contains pixel labels\n",
    "        loc = os.path.join('Images', *['data', 'labels', self.name])\n",
    "        with open(loc, 'rb') as f:\n",
    "            labelim = np.asarray(Image.open(f).convert('L'))\n",
    "        # The following line converts the image to binary 1,0\n",
    "        labelim = np.where(labelim == 255, 1, 0)\n",
    "        \n",
    "        # This gets what position we're looking at from the pixels array\n",
    "        pos = self.pixels[index]\n",
    "        \n",
    "        # Slices out the section of the image\n",
    "        # Transposes to be in the correct shape for torch tensoring.\n",
    "        patch = im[pos[0]-16:pos[0]+16,pos[1]-16:pos[1]+16]\n",
    "        #patch = patch.transpose((2,0,1))\n",
    "        #patch = torch.from_numpy(patch)\n",
    "        #patch = patch.float() # Cast to float type for network.\n",
    "        \n",
    "        # Gets the label\n",
    "        label = labelim[pos[0], pos[1]]\n",
    "        \n",
    "        # Return a Tuple!\n",
    "        if self.transform:\n",
    "            patch = self.transform(patch)\n",
    "\n",
    "        return (patch, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to define the transformations that turn the numpy patches into a tensor and then normalize them so that the mean is roughly 0.5. This ensures that rather than ranging from 0-255 the pixel values range roughly from 0 to 1. This increases the accuracy of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms the images into a nice format.\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Transformation object, converts to a tensor then normalizes.\n",
    "trans = transforms.Compose([transforms.ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PixelDataset('r_ut073249s57180.png', pixels, transform=trans)\n",
    "trainloader = DataLoader(data, batch_size=50, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the training method. It runs through the data, loads, finds the loss, and back propagates it. It prints the average loss for each individual batch after 50 batches. This should be pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, net, crit, opt, lr):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(loader, 0):\n",
    "        \n",
    "        # Get the input with their true labels\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Zeros the optimzer\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Get the predicted labels and find the loss by comparing\n",
    "        outputs = net(inputs)\n",
    "        loss = crit(outputs, labels)\n",
    "        \n",
    "        # Propagate the loss backwards\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Prints every 50 batches (Batch size currently 50, so every 2500 patches)\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:\n",
    "            print(running_loss)\n",
    "            print('[%d, %5d] Avg. loss: %.3f' % (epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network, the loss criterion, and the optimizer. I originally used a learning rate of 0.01, but that lead to an exploding gradient which would eventually culminate in many of the parameters rising up to be inf or nan. Reducing the global learning rate by a factor of ten solves this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_lr = 0.001\n",
    "net = CloudNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=global_lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently trains for two epochs, and technically only actually trains on a single image, which has something like 170,000 pixel patches. At present, anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.702518546953797\n",
      "[1,    50] Avg. loss: 0.434\n",
      "0.0763372658911976\n",
      "[1,   100] Avg. loss: 0.002\n",
      "0.007677087756746914\n",
      "[1,   150] Avg. loss: 0.000\n",
      "0.5932049435796216\n",
      "[1,   200] Avg. loss: 0.012\n",
      "6.959521498341928\n",
      "[1,   250] Avg. loss: 0.139\n",
      "13.420562566258013\n",
      "[1,   300] Avg. loss: 0.268\n",
      "31.329120349138975\n",
      "[1,   350] Avg. loss: 0.627\n",
      "15.48062838288024\n",
      "[1,   400] Avg. loss: 0.310\n",
      "9.4654719135724\n",
      "[1,   450] Avg. loss: 0.189\n",
      "5.4548792181303725\n",
      "[1,   500] Avg. loss: 0.109\n",
      "4.781893578823656\n",
      "[1,   550] Avg. loss: 0.096\n",
      "4.323571750428528\n",
      "[1,   600] Avg. loss: 0.086\n",
      "4.4150537091773\n",
      "[1,   650] Avg. loss: 0.088\n",
      "4.56357005238533\n",
      "[1,   700] Avg. loss: 0.091\n",
      "4.467688464093953\n",
      "[1,   750] Avg. loss: 0.089\n",
      "4.372548872604966\n",
      "[1,   800] Avg. loss: 0.087\n",
      "5.341641454375349\n",
      "[1,   850] Avg. loss: 0.107\n",
      "5.7558949190424755\n",
      "[1,   900] Avg. loss: 0.115\n",
      "4.699652400566265\n",
      "[1,   950] Avg. loss: 0.094\n",
      "4.551505705399904\n",
      "[1,  1000] Avg. loss: 0.091\n",
      "3.2589515046565793\n",
      "[1,  1050] Avg. loss: 0.065\n",
      "3.923074229445774\n",
      "[1,  1100] Avg. loss: 0.078\n",
      "5.089709387742914\n",
      "[1,  1150] Avg. loss: 0.102\n",
      "4.5704767361457925\n",
      "[1,  1200] Avg. loss: 0.091\n",
      "4.415217854373623\n",
      "[1,  1250] Avg. loss: 0.088\n",
      "3.5399942376243416\n",
      "[1,  1300] Avg. loss: 0.071\n",
      "2.6186637836508453\n",
      "[1,  1350] Avg. loss: 0.052\n",
      "2.7171138349949615\n",
      "[1,  1400] Avg. loss: 0.054\n",
      "2.0215502653372823\n",
      "[1,  1450] Avg. loss: 0.040\n",
      "1.6436845071439166\n",
      "[1,  1500] Avg. loss: 0.033\n",
      "1.7505101106107759\n",
      "[1,  1550] Avg. loss: 0.035\n",
      "2.6572543121510535\n",
      "[1,  1600] Avg. loss: 0.053\n",
      "2.094541381346062\n",
      "[1,  1650] Avg. loss: 0.042\n",
      "0.889334633906401\n",
      "[1,  1700] Avg. loss: 0.018\n",
      "0.7767426052159863\n",
      "[1,  1750] Avg. loss: 0.016\n",
      "1.3329274917286966\n",
      "[1,  1800] Avg. loss: 0.027\n",
      "1.2510606649320835\n",
      "[1,  1850] Avg. loss: 0.025\n",
      "1.859649137742963\n",
      "[1,  1900] Avg. loss: 0.037\n",
      "2.2623623335093725\n",
      "[1,  1950] Avg. loss: 0.045\n",
      "2.7087873572309036\n",
      "[1,  2000] Avg. loss: 0.054\n",
      "2.559945446701022\n",
      "[1,  2050] Avg. loss: 0.051\n",
      "4.955161322868662\n",
      "[1,  2100] Avg. loss: 0.099\n",
      "6.517715260008117\n",
      "[1,  2150] Avg. loss: 0.130\n",
      "5.313685977060231\n",
      "[1,  2200] Avg. loss: 0.106\n",
      "4.881632805423578\n",
      "[1,  2250] Avg. loss: 0.098\n",
      "2.5581140379399585\n",
      "[1,  2300] Avg. loss: 0.051\n",
      "3.377251878941024\n",
      "[1,  2350] Avg. loss: 0.068\n",
      "2.8797600034667994\n",
      "[1,  2400] Avg. loss: 0.058\n",
      "2.7943552206415916\n",
      "[1,  2450] Avg. loss: 0.056\n",
      "1.6238316131193642\n",
      "[1,  2500] Avg. loss: 0.032\n",
      "1.1513546180467529\n",
      "[1,  2550] Avg. loss: 0.023\n",
      "1.4847616314073093\n",
      "[1,  2600] Avg. loss: 0.030\n",
      "1.6008314442406117\n",
      "[1,  2650] Avg. loss: 0.032\n",
      "1.8954336883853102\n",
      "[1,  2700] Avg. loss: 0.038\n",
      "3.1039827567292377\n",
      "[1,  2750] Avg. loss: 0.062\n",
      "2.575454687634192\n",
      "[1,  2800] Avg. loss: 0.052\n",
      "3.4020376267326355\n",
      "[1,  2850] Avg. loss: 0.068\n",
      "4.408500787796584\n",
      "[1,  2900] Avg. loss: 0.088\n",
      "4.295198931620689\n",
      "[1,  2950] Avg. loss: 0.086\n",
      "2.1593304286143393\n",
      "[1,  3000] Avg. loss: 0.043\n",
      "2.07700828265115\n",
      "[1,  3050] Avg. loss: 0.042\n",
      "2.0222429548848595\n",
      "[1,  3100] Avg. loss: 0.040\n",
      "2.0334168571498594\n",
      "[1,  3150] Avg. loss: 0.041\n",
      "6.884911922170431\n",
      "[1,  3200] Avg. loss: 0.138\n",
      "6.7347672764281015\n",
      "[1,  3250] Avg. loss: 0.135\n",
      "0.1622800582117634\n",
      "[1,  3300] Avg. loss: 0.003\n",
      "0.18990411698177923\n",
      "[1,  3350] Avg. loss: 0.004\n",
      "5.985425584905897\n",
      "[2,    50] Avg. loss: 0.120\n",
      "0.07762593252118677\n",
      "[2,   100] Avg. loss: 0.002\n",
      "0.025842838309472427\n",
      "[2,   150] Avg. loss: 0.001\n",
      "0.556727386690909\n",
      "[2,   200] Avg. loss: 0.011\n",
      "5.262413968710462\n",
      "[2,   250] Avg. loss: 0.105\n",
      "8.667415662668645\n",
      "[2,   300] Avg. loss: 0.173\n",
      "11.826652497285977\n",
      "[2,   350] Avg. loss: 0.237\n",
      "5.627020950196311\n",
      "[2,   400] Avg. loss: 0.113\n",
      "5.575373616768047\n",
      "[2,   450] Avg. loss: 0.112\n",
      "4.501869859173894\n",
      "[2,   500] Avg. loss: 0.090\n",
      "3.506961961626075\n",
      "[2,   550] Avg. loss: 0.070\n",
      "2.417863289709203\n",
      "[2,   600] Avg. loss: 0.048\n",
      "2.548973243770888\n",
      "[2,   650] Avg. loss: 0.051\n",
      "3.118368080817163\n",
      "[2,   700] Avg. loss: 0.062\n",
      "2.5385940141859464\n",
      "[2,   750] Avg. loss: 0.051\n",
      "2.381211031868588\n",
      "[2,   800] Avg. loss: 0.048\n",
      "2.891332134095137\n",
      "[2,   850] Avg. loss: 0.058\n",
      "2.2784259563613887\n",
      "[2,   900] Avg. loss: 0.046\n",
      "2.141426925165433\n",
      "[2,   950] Avg. loss: 0.043\n",
      "2.9390734510211587\n",
      "[2,  1000] Avg. loss: 0.059\n",
      "1.8414224546431797\n",
      "[2,  1050] Avg. loss: 0.037\n",
      "2.653073695356852\n",
      "[2,  1100] Avg. loss: 0.053\n",
      "4.222769757001515\n",
      "[2,  1150] Avg. loss: 0.084\n",
      "4.024812665842546\n",
      "[2,  1200] Avg. loss: 0.080\n",
      "2.687552430550568\n",
      "[2,  1250] Avg. loss: 0.054\n",
      "2.1190811893457067\n",
      "[2,  1300] Avg. loss: 0.042\n",
      "2.099290921789361\n",
      "[2,  1350] Avg. loss: 0.042\n",
      "2.2201804093274404\n",
      "[2,  1400] Avg. loss: 0.044\n",
      "1.5864593730657361\n",
      "[2,  1450] Avg. loss: 0.032\n",
      "1.0679114262457006\n",
      "[2,  1500] Avg. loss: 0.021\n",
      "1.1825428139309224\n",
      "[2,  1550] Avg. loss: 0.024\n",
      "2.1706456346012146\n",
      "[2,  1600] Avg. loss: 0.043\n",
      "1.6906656796418247\n",
      "[2,  1650] Avg. loss: 0.034\n",
      "0.7312440819723633\n",
      "[2,  1700] Avg. loss: 0.015\n",
      "0.6079462382103884\n",
      "[2,  1750] Avg. loss: 0.012\n",
      "1.0908877036777085\n",
      "[2,  1800] Avg. loss: 0.022\n",
      "1.0940329039708558\n",
      "[2,  1850] Avg. loss: 0.022\n",
      "1.7849163938190031\n",
      "[2,  1900] Avg. loss: 0.036\n",
      "1.8635804443911184\n",
      "[2,  1950] Avg. loss: 0.037\n",
      "2.281743388812174\n",
      "[2,  2000] Avg. loss: 0.046\n",
      "2.121610200032592\n",
      "[2,  2050] Avg. loss: 0.042\n",
      "4.280705246252182\n",
      "[2,  2100] Avg. loss: 0.086\n",
      "5.149188498769945\n",
      "[2,  2150] Avg. loss: 0.103\n",
      "4.668289743967762\n",
      "[2,  2200] Avg. loss: 0.093\n",
      "3.604572689779161\n",
      "[2,  2250] Avg. loss: 0.072\n",
      "2.0086262044787873\n",
      "[2,  2300] Avg. loss: 0.040\n",
      "2.8929055597454862\n",
      "[2,  2350] Avg. loss: 0.058\n",
      "2.491958925145809\n",
      "[2,  2400] Avg. loss: 0.050\n",
      "2.3587236046059843\n",
      "[2,  2450] Avg. loss: 0.047\n",
      "1.3579133073926641\n",
      "[2,  2500] Avg. loss: 0.027\n",
      "0.7928443530872755\n",
      "[2,  2550] Avg. loss: 0.016\n",
      "1.2426660524761246\n",
      "[2,  2600] Avg. loss: 0.025\n",
      "1.2724690821432887\n",
      "[2,  2650] Avg. loss: 0.025\n",
      "1.4302881267431076\n",
      "[2,  2700] Avg. loss: 0.029\n",
      "2.165674677637071\n",
      "[2,  2750] Avg. loss: 0.043\n",
      "1.5416007496260136\n",
      "[2,  2800] Avg. loss: 0.031\n",
      "3.023089739754596\n",
      "[2,  2850] Avg. loss: 0.060\n",
      "3.9020752788583195\n",
      "[2,  2900] Avg. loss: 0.078\n",
      "3.350440722279018\n",
      "[2,  2950] Avg. loss: 0.067\n",
      "1.6869307775132256\n",
      "[2,  3000] Avg. loss: 0.034\n",
      "2.0998864606269763\n",
      "[2,  3050] Avg. loss: 0.042\n",
      "1.8269566104797832\n",
      "[2,  3100] Avg. loss: 0.037\n",
      "1.8998674193753686\n",
      "[2,  3150] Avg. loss: 0.038\n",
      "5.138116465037456\n",
      "[2,  3200] Avg. loss: 0.103\n",
      "3.321529893659317\n",
      "[2,  3250] Avg. loss: 0.066\n",
      "0.1575182336382568\n",
      "[2,  3300] Avg. loss: 0.003\n",
      "0.2064449113640876\n",
      "[2,  3350] Avg. loss: 0.004\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train(trainloader, net, criterion, optimizer, lr=global_lr)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network is trained and initiliazed, we can predict. The next cell predicts the image loaded into the valloader. It finds the accuracy and prints the total number of pixel patches. Following that we turn the array of predictions (preds) into a two dimensional binary image, where 1 represents a cloud pixel and 0 represents and non cloud pixel. Finally we then plot the two side by side to see how we did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = PixelDataset('r_ut041252s80160.png', pixels, transform=trans)\n",
    "valloader = DataLoader(data2, batch_size=50, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The array of predictions.\n",
    "preds = np.asarray([])\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# \n",
    "for data in valloader:\n",
    "    # The true labels and image data\n",
    "    images, labels = data\n",
    "        \n",
    "    # Gets the predicted labels\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "    # Adds to the total the number of labels here (should be equal to batch size)\n",
    "    total = total + labels.size(0)\n",
    "        \n",
    "    # If the predicted label is the same as the true label increment correct by 1\n",
    "    # This line checks all 50 patches in the batch at once.\n",
    "    correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    preds = np.concatenate((preds, predicted.numpy()))\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print('Total pixels analyzed: ' + str(total))\n",
    "print('Accuracy: '  + str(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = np.zeros((512,512))\n",
    "for i, loc in enumerate(pixels):\n",
    "    preds2[loc[0], loc[1]] = preds[i]\n",
    "\n",
    "name = 'r_ut041252s80160.png'\n",
    "loc = os.path.join('Images', *['data', 'train', 'new', name])\n",
    "loc2 = os.path.join('Images', *['data', 'labels', name])\n",
    "\n",
    "with open(loc2, 'rb') as f:\n",
    "    labelim = np.asarray(Image.open(f).convert('L'))\n",
    "\n",
    "with open(loc, 'rb') as f:\n",
    "    realim = np.asarray(Image.open(f).convert('L'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,3)\n",
    "ax[0].imshow(preds2, cmap='gray')\n",
    "ax[0].set_title('Predicted Cloud Map')\n",
    "ax[1].imshow(labelim, cmap='gray')\n",
    "ax[1].set_title('Validation Cloud Map')\n",
    "ax[2].imshow(realim, cmap='gray')\n",
    "ax[2].set_title('Actual Image')\n",
    "\n",
    "fig.set_size_inches(17, 8)\n",
    "for a in ax:\n",
    "    a.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
